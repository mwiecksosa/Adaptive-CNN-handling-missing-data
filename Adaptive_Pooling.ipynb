{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Adaptive_Pooling.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zedwzRMeaLt0","colab_type":"text"},"source":["#Load the data and do pre-processing"]},{"cell_type":"code","metadata":{"id":"t77oXj8paOX5","colab_type":"code","outputId":"6436aa68-2222-413e-c02d-f08306b32abd","executionInfo":{"status":"ok","timestamp":1562961575994,"user_tz":240,"elapsed":3356,"user":{"displayName":"Yujuan Fu","photoUrl":"https://lh3.googleusercontent.com/-aSatPlH9G3E/AAAAAAAAAAI/AAAAAAAAAAc/40s3cQJg7Zc/s64/photo.jpg","userId":"11339928270579390295"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","def impute_missing_values(X):\n","    \"\"\"\n","    For each feature column, impute missing values  (np.nan) with the \n","    population mean for that feature.\n","    \n","    Args:\n","        X: np.array, shape (N, L, d). X could contain missing values\n","    Returns:\n","        X: np.array, shape (N, L, d). X does not contain any missing values\n","    \"\"\"\n","    N, L, d = X.shape\n","    X = X.reshape(N*L, d)\n","    col_means = np.nanmean(X, axis=0)\n","    inds = np.where(np.isnan(X))\n","    X[inds] = np.take(col_means, inds[1])\n","    return X.reshape(N, L, d)\n","  \n","def standardize_features(X):\n","    \"\"\"\n","    For each feature column, normalize all values to range [0, 1].\n","    Args:\n","        X: np.array, shape (N, d).\n","    Returns:\n","        X: np.array, shape (N, d). Values are normalized per column.\n","    \"\"\"\n","    scaler = StandardScaler()\n","    N, L, d = X.shape\n","    X = X.reshape(N*L, d)\n","    X = scaler.fit_transform(X)\n","    return X.reshape(N, L, d)\n","\n","#with open(\"features_labels_12000.p\", 'rb') as f:\n","#      features, df_labels = pickle.load(f)\n","#      X = np.array([df_i.values for df_i in features])[0:10000,:,:]\n","#      y = df_labels['In-hospital_death'].values.copy()[0:10000]\n","#      y[y == -1] = 0\n","\n","#np.save(\"X.npy\",X)\n","\n","X=np.load(\"X.npy\")\n","y=np.load(\"y.npy\")\n","\n","X_train1,X_train2,X_test1,X_test2= X[0:8000,:,0:35],X[0:8000,:,35:70], X[8000:10000,:,0:35],X[8000:10000,:,35:70]\n","y_train,y_test=y[0:8000],y[8000:10000]\n","\n","X_train1,X_test1=impute_missing_values(X_train1),impute_missing_values(X_test1)\n","X_train1,X_test1=standardize_features(X_train1),standardize_features(X_test1)\n","print(np.shape(X))\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["(10000, 48, 70)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1HYBz2h1aeSI","colab_type":"text"},"source":["# Defining layers"]},{"cell_type":"code","metadata":{"id":"KHcwC4kmag-J","colab_type":"code","outputId":"dc8924e4-53f5-4304-ec98-66542ee8e08f","executionInfo":{"status":"ok","timestamp":1562961857380,"user_tz":240,"elapsed":161291,"user":{"displayName":"Yujuan Fu","photoUrl":"https://lh3.googleusercontent.com/-aSatPlH9G3E/AAAAAAAAAAI/AAAAAAAAAAc/40s3cQJg7Zc/s64/photo.jpg","userId":"11339928270579390295"}},"colab":{"base_uri":"https://localhost:8080/","height":339}},"source":["import random\n","print (\"Random number with seed 0\")\n","random.seed(0)\n","def transfer_y(y):\n","  size=np.shape(y)[0]\n","  result=np.zeros((size,2))\n","  for i in range(size):\n","    if y[i]==1:\n","      result[i,1]=1\n","    else:\n","      result[i,0]=1\n","  return result\n","tf.reset_default_graph() \n","#[1, filter_width, in_channels, out_channels]\n","weights = {\n","    'wc1': tf.get_variable('W0', shape=(3,35,32), initializer=tf.contrib.layers.xavier_initializer()), \n","    'wc2': tf.get_variable('W1', shape=(3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n","    'wc3': tf.get_variable('W2', shape=(3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n","    'wd1': tf.get_variable('W3', shape=(6*8*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n","    'out': tf.get_variable('W6', shape=(128,2), initializer=tf.contrib.layers.xavier_initializer()), \n","}\n","biases = {\n","    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n","    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n","    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n","    'bd1': tf.get_variable('B3', shape=(128),initializer=tf.contrib.layers.xavier_initializer()),\n","    'out': tf.get_variable('B4', shape=(2), initializer=tf.contrib.layers.xavier_initializer()),\n","}\n","training_iters = 200 \n","learning_rate = 0.001 \n","batch_size = 64\n","n_classes=2\n","\n","\n","x = tf.placeholder(\"float\", [batch_size, 48, 35])   #input_length, dimension\n","x_missing = tf.placeholder(\"float\", [batch_size, 48, 35])\n","y = tf.placeholder(\"float\", [batch_size,2])\n","\n","def conv1d(x, W, b,if_bias=True):\n","    # Conv2D wrapper, with bias and relu activation\n","    x = tf.nn.conv1d(x, W, padding='SAME')\n","    if if_bias:\n","      x = tf.nn.bias_add(x, b)\n","    return tf.nn.relu(x) \n","\n","def maxpool1d(x,k):\n","    return tf.nn.max_pool1d(x, ksize=k, strides=1,padding='SAME')\n","\n","\n","\n","def conv_net(x,x_missing, weights, biases):  \n","    conv1 = conv1d(x, weights['wc1'], biases['bc1'])\n","    #originally, dimention of 48\n","    #adding an average/abs matrix\n","    conv1 = tf.nn.avg_pool1d(conv1,  ksize=2, strides=1,padding='SAME')\n","    conv2 = conv1d(conv1, weights['wc2'], biases['bc2'])\n","    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n","    conv2 = maxpool1d(conv2, k=2)\n","    \n","    conv3 = conv1d(conv2, weights['wc3'], biases['bc3'])\n","    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n","    conv3 = maxpool1d(conv3, k=2)\n","    #print(np.shape(conv3))\n","    # Fully connected layer\n","    # Reshape conv2 output to fit fully connected layer input\n","    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n","    #print(np.shape(fc1))\n","    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n","    fc1 = tf.nn.relu(fc1)\n","    \n","    # Output, class prediction\n","    # finally we multiply the fully connected layer with the weights and add a bias term. \n","    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n","    return out\n","  \n","pred = conv_net(x,x_missing, weights, biases)\n","#print(np.shape(pred),np.shape(y))\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n","with tf.Session() as sess:\n","    sess.run(init) \n","\n","    train_loss = []\n","    test_loss = []\n","    train_accuracy = []\n","    test_accuracy = []\n","    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n","    x_len=len(y_train)\n","    for i in range(training_iters):\n","        for batch in range(x_len//batch_size):\n","            batch_x = X_train1[batch*batch_size:min((batch+1)*batch_size,x_len),:,:]\n","            batch_x_missing = X_train2[batch*batch_size:min((batch+1)*batch_size,x_len),:,:]\n","            #print(np.shape(batch_x_missing))\n","            batch_y=transfer_y(y_train[batch*batch_size:min((batch+1)*batch_size,x_len)] )\n","            # Run optimization op (backprop).\n","                # Calculate batch loss and accuracy\n","            \n","            opt = sess.run(optimizer, feed_dict={x: batch_x,x_missing: batch_x_missing,\\\n","                                                              y: batch_y})\n","            \n","            loss, auc = sess.run([cost, accuracy], feed_dict={x: batch_x,x_missing: batch_x_missing,\\\n","                                                              y: batch_y})\n","        #print(\"Iter \" + str(i) + \", Loss= \" +\"{:.6f}\".format(loss) + \", Accuracy= \" + \"{:.5f}\".format(auc))\n","        #print(\"Optimization Finished!\")\n","\n","        # Calculate accuracy for all 10000 mnist test images\n","        test_auc,valid_loss =0,0\n","        for k in range(25):\n","          from random import sample \n","          index=np.array(range(64))+k*64\n","          temp1,temp2= sess.run([accuracy,cost], feed_dict={x: X_test1[index,:,:],x_missing: X_test2[index,:,:], y :transfer_y(y_train[index])})\n","          test_auc=test_auc+temp1\n","          valid_loss =valid_loss+temp2\n","        test_auc=test_auc/25\n","        valid_loss =valid_loss/25\n","        train_loss.append(loss)\n","        test_loss.append(valid_loss)\n","        train_accuracy.append(auc)\n","        test_accuracy.append(test_auc)\n","        if test_auc>test_accuracy[-1] and  test_auc>test_accuracy[-2]:\n","          print(\"Testing Accuracy:\",\"{:.5f}\".format(test_auc))\n","    summary_writer.close()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Random number with seed 0\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0712 20:01:38.389577 140363388868480 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0712 20:01:38.511859 140363388868480 deprecation.py:323] From <ipython-input-4-543b079efd9e>:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rahjZnJGahz0","colab_type":"text"},"source":["# training the data"]},{"cell_type":"markdown","metadata":{"id":"glCuZ8d_bTvx","colab_type":"text"},"source":["tutorial:\n","https://www.datacamp.com/community/tutorials/cnn-tensorflow-python\n","\n","costumized layer\n","\n","https://www.tensorflow.org/tutorials/eager/custom_layers\n","\n","reliability based on missingness and distance based on time.\n","\n"]},{"cell_type":"code","metadata":{"id":"Cz71kguS1MbZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0x_ZpNStal5V","colab_type":"code","outputId":"21fe6bfb-72c6-4623-f9c3-eb7c0922d709","executionInfo":{"status":"ok","timestamp":1562962160067,"user_tz":240,"elapsed":6449,"user":{"displayName":"Yujuan Fu","photoUrl":"https://lh3.googleusercontent.com/-aSatPlH9G3E/AAAAAAAAAAI/AAAAAAAAAAc/40s3cQJg7Zc/s64/photo.jpg","userId":"11339928270579390295"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["import random\n","print (\"Random number with seed 0\")\n","random.seed(0)\n","def transfer_y(y):\n","  size=np.shape(y)[0]\n","  result=np.zeros((size,2))\n","  for i in range(size):\n","    if y[i]==1:\n","      result[i,1]=1\n","    else:\n","      result[i,0]=1\n","  return result\n","tf.reset_default_graph() \n","#[1, filter_width, in_channels, out_channels]\n","weights = {\n","    'wc1': tf.get_variable('W0', shape=(3,35,32), initializer=tf.contrib.layers.xavier_initializer()), \n","    'wc2': tf.get_variable('W1', shape=(3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n","    'wc3': tf.get_variable('W2', shape=(3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n","    'wd1': tf.get_variable('W3', shape=(6*8*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n","    'out': tf.get_variable('W6', shape=(128,2), initializer=tf.contrib.layers.xavier_initializer()), \n","}\n","biases = {\n","    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n","    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n","    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n","    'bd1': tf.get_variable('B3', shape=(128),initializer=tf.contrib.layers.xavier_initializer()),\n","    'out': tf.get_variable('B4', shape=(2), initializer=tf.contrib.layers.xavier_initializer()),\n","}\n","training_iters = 200 \n","learning_rate = 0.001 \n","batch_size = 64\n","n_classes=2\n","\n","\n","x = tf.placeholder(\"float\", [batch_size, 48, 35])   #input_length, dimension\n","x_missing = tf.placeholder(\"float\", [batch_size, 48, 35])\n","y = tf.placeholder(\"float\", [batch_size,2])\n","\n","def conv1d(x, W, b,if_bias=True):\n","    # Conv2D wrapper, with bias and relu activation\n","    x = tf.nn.conv1d(x, W, padding='SAME')\n","    if if_bias:\n","      x = tf.nn.bias_add(x, b)\n","    return tf.nn.relu(x) \n","\n","def maxpool1d(x,k):\n","    return tf.nn.max_pool1d(x, ksize=k, strides=1,padding='SAME')\n","\n","\n","\n","def conv_net(x,x_missing, weights, biases):  \n","    conv1 = conv1d(x, weights['wc1'], biases['bc1'])\n","    #originally, dimention of 48\n","    #adding an average/abs matrix\n","    conv_missing=conv1d(x_missing,weights['wc1'],biases['bc1'],False)\n","    \n","    w=1-tf.math.abs(tf.math.l2_normalize(conv_missing))\n","    conv1 = tf.nn.avg_pool1d(tf.math.multiply(conv1,w),  ksize=2, strides=1,padding='SAME')\n","   \n","    conv2 = conv1d(conv1, weights['wc2'], biases['bc2'])\n","    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n","    conv2 = maxpool1d(conv2, k=2)\n","    \n","    conv3 = conv1d(conv2, weights['wc3'], biases['bc3'])\n","    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n","    conv3 = maxpool1d(conv3, k=2)\n","    #print(np.shape(conv3))\n","    # Fully connected layer\n","    # Reshape conv2 output to fit fully connected layer input\n","    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n","    #print(np.shape(fc1))\n","    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n","    fc1 = tf.nn.relu(fc1)\n","    \n","    # Output, class prediction\n","    # finally we multiply the fully connected layer with the weights and add a bias term. \n","    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n","    return out\n","  \n","pred = conv_net(x,x_missing, weights, biases)\n","#print(np.shape(pred),np.shape(y))\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","\n","init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n","with tf.Session() as sess:\n","    sess.run(init) \n","\n","    train_loss = []\n","    test_loss = []\n","    train_accuracy = []\n","    test_accuracy = []\n","    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n","    x_len=len(y_train)\n","    for i in range(training_iters):\n","        for batch in range(x_len//batch_size):\n","            batch_x = X_train1[batch*batch_size:min((batch+1)*batch_size,x_len),:,:]\n","            batch_x_missing = X_train2[batch*batch_size:min((batch+1)*batch_size,x_len),:,:]\n","            #print(np.shape(batch_x_missing))\n","            batch_y=transfer_y(y_train[batch*batch_size:min((batch+1)*batch_size,x_len)] )\n","            # Run optimization op (backprop).\n","                # Calculate batch loss and accuracy\n","            \n","            opt = sess.run(optimizer, feed_dict={x: batch_x,x_missing: batch_x_missing,\\\n","                                                              y: batch_y})\n","            \n","            loss, auc = sess.run([cost, accuracy], feed_dict={x: batch_x,x_missing: batch_x_missing,\\\n","                                                              y: batch_y})\n","        #print(\"Iter \" + str(i) + \", Loss= \" +\"{:.6f}\".format(loss) + \", Accuracy= \" + \"{:.5f}\".format(auc))\n","        #print(\"Optimization Finished!\")\n","\n","        # Calculate accuracy for all 10000 mnist test images\n","        test_auc,valid_loss =0,0\n","        for k in range(25):\n","          from random import sample \n","          index=np.array(range(64))+k*64\n","          temp1,temp2= sess.run([accuracy,cost], feed_dict={x: X_test1[index,:,:],x_missing: X_test2[index,:,:], y :transfer_y(y_train[index])})\n","          test_auc=test_auc+temp1\n","          valid_loss =valid_loss+temp2\n","        test_auc=test_auc/25\n","        valid_loss =valid_loss/25\n","        train_loss.append(loss)\n","        test_loss.append(valid_loss)\n","        train_accuracy.append(auc)\n","        test_accuracy.append(test_auc)\n","        if i >3:\n","          if test_auc<test_accuracy[-3] and  test_auc<test_accuracy[-2]:\n","            print(\"Testing Accuracy:\",\"{:.5f}\".format(max(test_accuracy)))\n","            break\n","    summary_writer.close()\n","          "],"execution_count":11,"outputs":[{"output_type":"stream","text":["Random number with seed 0\n","0.75625 0.756875\n","Testing Accuracy: 0.80812\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NT4FX5fe2tHX","colab_type":"code","colab":{}},"source":["from numpy.linalg import norm\n","X=np.zeros((2,2,3))\n","X[1,:,:]=[[2,-5,8],[4,2,0]]\n","X[0,:,:]=[[2,5,1],[4,2,8]]\n","\n","def norm_to_1(X):\n","  w=tf.Session().run(X)\n","  size=np.shape(X)[2]\n","  for i in range(size):\n","    summ=np.sum(X[:,:,i])\n","    if not summ==0:\n","       X[:,:,i]=X[:,:,i]/summ\n","  return X\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-tzs2uCl2y5","colab_type":"code","outputId":"aba0e32e-9ec0-4627-e710-86b4a69f9f49","executionInfo":{"status":"ok","timestamp":1562874705555,"user_tz":240,"elapsed":1309,"user":{"displayName":"Yujuan Fu","photoUrl":"https://lh3.googleusercontent.com/-aSatPlH9G3E/AAAAAAAAAAI/AAAAAAAAAAc/40s3cQJg7Zc/s64/photo.jpg","userId":"11339928270579390295"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y=np.load(\"y.npy\")\n","\n","X_train1,X_train2,X_test1,X_test2= X[0:8000,:,0:35],X[0:8000,:,35:70], X[8000:10000,:,0:35],X[8000:10000,:,35:70]\n","y_train,y_test=y[0:8000],y[8000:10000]\n","\n","X_train1,X_test1=impute_missing_values(X_train1),impute_missing_values(X_test1)\n","X_train1,X_test1=standardize_features(X_train1),standardize_features(X_test1)\n","print(np.shape(X))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(10000, 48, 70)\n"],"name":"stdout"}]}]}